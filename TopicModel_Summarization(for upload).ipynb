{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For wide monitor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conncet Severance hospital DB\n",
    "import MySQLdb\n",
    "db = MySQLdb.connect(host='103.22.220.149', #host\n",
    "                    user='hoheon', # user ID\n",
    "                    passwd='yonsei2018!', # password\n",
    "                    db='food', # database name\n",
    "                    use_unicode=True, # to define unicode\n",
    "                    charset='utf8') # to define unicode\n",
    "\n",
    "# make cursor object to excute all the SQL\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_sql = \"SELECT * FROM EXAMPLE;\" \n",
    "cur.execute(select_sql)\n",
    "result = cur.fetchall()\n",
    "df = pd.DataFrame(np.array(result))\n",
    "df.to_csv('scraping.csv')\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "import pandas as pd\n",
    "df= pd.read_csv('scraping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "df.columns = ['INDEX','ID', 'KCD_CODE', 'TITLE', 'DATE', 'CONTENTS', 'PROVIDER', 'WRITER']\n",
    "sample='''[현대건강신문=채수정 기자] 날씨가 추운 겨울철은 고혈압 환자들의 건강관리에 더 주의를 기울여야 한다. 혈압은 상황에 따라 변하는데 특히 기온에 민감하게 반응한다. 몸이 찬 공기에 노출되면서 교감신경계 영향으로 혈관이 수축하기 때문이다. 연구에 따르면 기온이 1도 내려가면 수축기혈압은 1.3㎜Hg 상승한다고 알려져 있다.\n",
    "경희의료원 심장혈관센터 김원 교수는 “추위에 혈압 상승이 무서운 이유는 고혈압 자체보다 뇌출혈, 심근경색증, 뇌졸중 등의 합병증의 위험성이 크게 높아지기 때문”이라고 말한다.또한 겨울철은 실내 생활이 늘면서 운동량이 줄어들어 식생활 등 생활 관리에 더욱 신경써야 한다. 우리나라 고혈압 환자들에게 가장 어려운 것 중 하나가 바로 짜게 먹는 식습관이다. 소금을 줄여야 하지만, 오랫동안 익숙해진 식습관을 고치기는 쉽지 않다. \n",
    "고혈압, 당뇨 등 만성질환자들은 물론 적게 먹고 운동을 많이 하는 것이 가장 좋지만, 어떤 식품을 먹느냐 하는 것도 중요하다. 고혈압 환자들에게는 몸속의 나트륨을 배출하는 데 도움을 주는 칼륨이 풍부한 식품을 섭취하는 것이 도움이 된다.추운 겨울철 고혈압 환자들의 건강관리에 도움을 주는 칼륨이 풍부한 식품을 알아보았다.양배추=양배추는 우리 주위에서 가장 흔하고 저렴하게 먹을 수 있는 식재료라 무심코 지나치기 쉽지만, 최고의 슈퍼푸드다. \n",
    "흔히 위에 좋은 식품으로 잘 알려져 있는 양배추는 위를 보호하는 것 외에 비타민A와 칼슘, 철분, 칼륨이 풍부해 면역력을 강화하고, 항암효과도 있다. 또 저열량에 식이섬유도 풍부해 다이어트에도 효과만점이다.저장성이 좋아 겨울철에 구하기 쉬운 양배추는 저열량에 칼륨 함량도 높아 혈압 관리에도 도움이 된다. 바나나=바나나는 칼륨 함량이 높은 것으로 이미 잘 알려져 있다. 간편하게 먹을 수 있고, 쉽게 접할 수 있는 바나나는 열량이 낮고 1개당 422mg의 칼륨이 함유돼 있다. \n",
    "바나나의 칼륨은 나트륨 배출을 돕고 전해질 균형을 맞춰 혈압을 정상 수준으로 유지시킨다. 또 바나나의 펙틴 성분은 변비 개선에 도움을 주고, 마그네슘 성분도 풍부해 수분 균형을 유지시키고 눈 떨림 증상을 완화한다. \n",
    "시금치=겨울에 제철을 맞은 식재료 중 하나인 시금치도 칼륨이 풍부하다. 시금치 100g에는 약 800mg의 칼륨을 함유하고 있어 가격대비 최고다. 시금치에는 또  철, 인, 칼슘 등의 미네랄과 녹황색 채소 중에 비타민 A가 가장 많고 카로틴함량이 높아 이를 많이 섭취하면 암 발병을 억제하는 데 효과가 있다. 이 밖에 사포닌과 질 좋은 섬유질이 들어 있어 변비에 효과가 있고 철분과 엽산 덕분에 빈혈 예방에도 좋다.\n",
    "우유=완전 식품으로 불리는 우유도 칼륨 함량이 높다. 일본총합과학대학 건강영양과학과 나가코 오쿠다 교수에 따르면, 일본인을 대상으로 한 연구에서 우유를 많이 마시면 남녀 모두에서 수축기(최대) 혈압이 눈에 띄게 낮아지는 것으로 나타났다.\n",
    "우유가 고혈압 예방에 효과적인 것은 우유에 함유된 칼륨·칼슘·마그네슘이 혈압을 낮추기 때문이다.오쿠다 교수는 우유를 즐겨 마시면 소금을 적게 섭취하게 돼, 결과적으로 혈압이 낮아진다고 설명했다.고구마=겨울철 영양만점 간식으로 사랑받는 고구마도 칼륨이 풍부한 식품이다. 다이어트 식품으로도 사랑 받는 고구마는 작은 주먹 정도 크기에 약 540mg의 칼륨이 함유돼 있다. 특히 고구마는 식이섬유도 풍부해 장 활동을 개선하고 달콤한 맛에 칼로리도 낮다.\n",
    "칼륨 성분은 우리 몸속에 나트륨 배출을 도와 혈압을 안정시키는데 도움을 준다. 다만 고혈압이나 심장 질환에 흔히 쓰이는 약인 베타차단제를 먹고 있거나 신장 건강이 나쁘다면 칼륨의 지나친 섭취에 주의해야한다. \n",
    "체내 칼륨 수치가 지나치게 높을 경우 신장 기능이 약한 신장질환자들의 경우 혈액에 쌓여 심장 기능을 떨어뜨리기 때문이다. 또 시금치나 양배추 등 비타민K 함량이 높은 식품들은 비타민 K 길항제를 복용하는 환자들이 주의해야한다.'''\n",
    "summarize(sample, ratio=0.25, word_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "sys.path.append('/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner')\n",
    "BASE_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/TF_IDF/'\n",
    "BASE_PATH2 = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner/'\n",
    "import pyTextMiner as ptm\n",
    "TOPIC_MODELING_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/Topic modeling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.loc[:80000, 'CONTENTS']\n",
    "# 개행문자 제거\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'example.txt', mode='w')\n",
    "for doc in documents:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full txt file\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'example.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "# SAVE\n",
    "import json\n",
    "outputfile = open('preprocessing_result_11K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 80000 txt file\n",
    "documents = df.loc[:80000, 'CONTENTS']\n",
    "# 개행문자 제거\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'example.txt', mode='w')\n",
    "for doc in documents:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'example.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "# SAVE\n",
    "import json\n",
    "outputfile = open('preprocessing_result_8K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to len 1 list\n",
    "word_list = []\n",
    "for sub in result:\n",
    "        word_list.extend(sub)\n",
    "\n",
    "# Word2Vec Model training\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(word_list, min_count=5, size = 100, window = 5, workers= 45, # ignore the voca that appear at least 5 times in the corpus\n",
    "                    sg=0,  # 0 : CBOW, 1: Skip-gram\n",
    "                    hs=0,  # 0: non-zero, neative sampling 1: hierachila softmax \n",
    "                    alpha= 0.01,\n",
    "                    min_alpha= 0.0001,\n",
    "                    max_vocab_size = 1000\n",
    "                   ) \n",
    "model.train(documents, total_examples=len(word_list), epochs=100, report_delay=0.1)\n",
    "\n",
    "# Average sentence vector\n",
    "index2word_set = set(model.wv.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.index2entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caculate similariry\n",
    "from scipy import spatial\n",
    "# from spatial import distance\n",
    "s1_afv = avg_feature_vector(df.CONTENTS[1], model=model, num_features=100, index2word_set=index2word_set)\n",
    "s2_afv = avg_feature_vector(df.CONTENTS[2], model=model, num_features=100, index2word_set=index2word_set)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Topic Modeling : Latent Dirichlet Allocation\n",
    "Topic modeling은 단어의 군집화. 기술적 분석임(Descriptive analaytics)\n",
    "전체의 문헌의 N개의 토픽을 구성하는 단어를 추출하는 방법. 사후 추론방법. \n",
    "* D: 문서 전체의 개수\n",
    "* K: 전체 토픽 수 (하이퍼 파라미터)\n",
    "* N: d번째 문서의 단어수 (W)\n",
    "* Z(d,n) : \n",
    "* Phi: Term Topic matrix (하이퍼파라미터 beta에 영향을 받는다. phi는 디리클레분포를 따른다는 가정)\n",
    "* Ceta(d,K): d번째 문헌에서 k번째 토픽의 확률들.\n",
    "* W(d,n): d번째 문헌에 있는, n번째의 단어\n",
    "* beta: 배타. \n",
    "\n",
    "\n",
    "문헌은 여러개의 topic으로 이루어져있다는게 가정임\n",
    "최적화하는 K값은 토픽의의 similarity는 낮아야한다.\n",
    "각각의 토픽끼리는 cohesive해야한다.\n",
    "Variannal inference: KL divergence을 계산해서 (dissimilarity을 찾는다)\n",
    "\n",
    "* perplexity가 낮을 수록 좋음 (순도): 늘 좋진 않다. 주관적으로 할 필요가 있음.\n",
    "* 잘 나온 토픽들로만 생성한다는게 이해가 안안되는데\n",
    "\n",
    "*LDA inference: 새로운 문헌이 들어올 때, topic을 추론하는 방법\n",
    "*LDA: word x topic * topic x document\n",
    "\n",
    "토픽을 추정하는 방법\n",
    "* Variational inference: 시간이 오래걸려서 잘 안씀.\n",
    "* Gibbs sampling: 토픽을 추론하기위해서, 깁스 방법으로 샘플링해서, 샘플링 한 단어를가지고 추론을 함. 확률적 샘플링이라 조금씩 달라질 수 있다.\n",
    "\n",
    "1. 포아송 분포로부터 문헌의 길이를 N을 선택\n",
    "2. 알파를 매개변수로 하는 주제 분포를 ceta을 선택한다 (alpha: 알아서 학습을 통해 추정이 된다)\n",
    "3. 임의로 주제를 정해서 \n",
    "\n",
    "다항 토픽모델링:  T*F: feature space을 변수로 넣고, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "import json\n",
    "with open('preprocessing_result_80K.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "        \n",
    "# Convert input data format\n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "K=20\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "from tqdm import tnrange\n",
    "for d in tnrange(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def document_count(d, documents, document_topics):\n",
    "    word_list_in_documents = documents[d]\n",
    "    topic = document_topics[d]\n",
    "    global document_topic_counts\n",
    "    global topic_word_counts\n",
    "    global topic_counts\n",
    "    for word in word_list_in_documents:\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "    print(d, word, topic)\n",
    "#     return (document_topic_counts, topic_word_counts, topic_counts)\n",
    "    \n",
    "    # topic_counts: 토픽별로 있는 단어의 빈도\n",
    "    # document_topic_counts: 각 토픽별로 있는 단어의 개수. 위의 토픽이 될만한 단어들을 카운트\n",
    "    # topic_counts: topic 에따른 총 단어수 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = multiprocessing.cpu_count() - 3 ; print(num_cores) # use 53 CPUs / total 56 CPU \n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(document_count)(d, word, topic) \\\n",
    "                                     for d in range(1) \\\n",
    "                                     for word, topic in zip(documents[d], document_topics[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "num_iteration = 100\n",
    "\n",
    "def processInput(i, *my_tuple):\n",
    "    global document_topic_counts\n",
    "    global topic_word_counts\n",
    "    global topic_counts\n",
    "    global document_lengths\n",
    "    global new_topic\n",
    "    document_topic_counts[d][topic] -= 1\n",
    "    topic_word_counts[topic][word] -= 1\n",
    "    topic_counts[topic] -= 1\n",
    "    document_lengths[d] -= 1\n",
    "    new_topic = choose_new_topic(d, word)\n",
    "    document_topics[d][i] = new_topic\n",
    "    document_topic_counts[d][new_topic] += 1\n",
    "    topic_word_counts[new_topic][word] += 1\n",
    "    topic_counts[new_topic] += 1\n",
    "    document_lengths[d] += 1\n",
    " \n",
    "num_cores = multiprocessing.cpu_count() - 3 # total 56 CPU\n",
    "print(num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processInput(1, (wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "import json\n",
    "with open('preprocessing_result_80K.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "        \n",
    "# Convert input data format\n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list \n",
    "\n",
    "random.seed(0)\n",
    "K=20\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "from tqdm import tnrange\n",
    "for iter in tnrange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "            \n",
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})\n",
    "import json\n",
    "outputfile = open('TopicModeling_80K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(i, d, *my_tuple):\n",
    "    '''\n",
    "    input: i: Sequence\n",
    "    my_tuple: enumerate(zip(documents[d], document_topics[d]))\n",
    "    '''\n",
    "    document_topic_counts[d][topic] -= 1\n",
    "    topic_word_counts[topic][word] -= 1\n",
    "    topic_counts[topic] -= 1\n",
    "    document_lengths[d] -= 1\n",
    "    new_topic = choose_new_topic(d, word)\n",
    "    document_topics[d][i] = new_topic\n",
    "    document_topic_counts[d][new_topic] += 1\n",
    "    topic_word_counts[new_topic][word] += 1\n",
    "    topic_counts[new_topic] += 1\n",
    "    document_lengths[d] += 1\n",
    "    print(document_lengths)\n",
    "#     print(dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topic_word_counts)):\n",
    "    print('{}th topic'.format(i), '-'*150)\n",
    "    print(sorted(dict(topic_word_counts[i]).items(), key=lambda x:x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 번문서의 topic의 빈도\n",
    "print(sorted(document_topic_counts[0].items(), key= lambda x:x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 번문서의 top one pick 주제\n",
    "import numpy as np\n",
    "empty_topic_list = []\n",
    "for i in tnrange(len(documents)):\n",
    "    try:\n",
    "        empty_topic_list.append(sorted(document_topic_counts[i].items(), key= lambda x:x[1], reverse=True)[0:1][0][0])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "outputfile = open('topic', 'w+')\n",
    "json.dump(topic_word_counts, outputfile)\n",
    "output_file = open('doc_topic_list', 'w+')\n",
    "json.dump(empty_topic_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('doc_topic_list', 'r') as f:\n",
    "    my_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k \n",
    "[j for sub in result[2:3] for term in sub for j in term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Filtered topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword 수정\n",
    "f = open(BASE_PATH2+'stopwordsKor.txt','a')\n",
    "for stopword_add in sorted_x[:30]:\n",
    "    f.write(stopword_add[0]+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic 별 단어 리스트\n",
    "# 전처리 리스트 불러오기\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "sys.path.append('/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner')\n",
    "BASE_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/TF_IDF/'\n",
    "BASE_PATH2 = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner/'\n",
    "import pyTextMiner as ptm\n",
    "TOPIC_MODELING_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/Topic modeling/'\n",
    "os.chdir(TOPIC_MODELING_PATH)\n",
    "\n",
    "\n",
    "with open('topic', 'r') as f:\n",
    "    topic = json.load(f)\n",
    "with open('doc_topic_list', 'r') as f:\n",
    "    topic_label = json.load(f)\n",
    "\n",
    "df= pd.read_csv('scraping.csv')\n",
    "# topic\n",
    "df['topic'] = np.array(topic_label)[:len(df)]\n",
    "reduced_df = df[:80000]\n",
    "reduced_df.columns = ['INDEX','ID', 'KCD_CODE', 'TITLE', 'DATE', 'CONTENTS', 'PROVIDER', 'WRITER','TOPIC']\n",
    "# 기사 필터링----------\n",
    "# 9번: 지역사회관리\n",
    "# 10번: 식이요법\n",
    "# 19번: 일반 고혈압정보\n",
    "reduced_df = reduced_df.loc[(reduced_df.TOPIC == 9) | (reduced_df.TOPIC == 10) |(reduced_df.TOPIC == 19)]\n",
    "reduced_df =  reduced_df.loc[:,'CONTENTS']\n",
    "reduced_df = reduced_df.values\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'reduced_news_result.txt', mode='w')\n",
    "for doc in reduced_df:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()\n",
    "# 전처리\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'reduced_news_result.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "outputfile = open('filter_news_preprocessed.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서프토픽모델링 \n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "random.seed(0)\n",
    "K=10\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "from tqdm import tnrange\n",
    "for d in tnrange(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "        \n",
    "        \n",
    "from tqdm import tnrange\n",
    "for iter in tnrange(1500):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "outputfile = open('TopicModeling_80K.json', 'w+')\n",
    "json.dump(obj_11K, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topic_word_counts)):\n",
    "    print('{}th topic'.format(i), '-'*150)\n",
    "    print(sorted(dict(topic_word_counts[i]).items(), key=lambda x:x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
