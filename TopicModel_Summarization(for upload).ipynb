{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For wide monitor\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conncet Severance hospital DB\n",
    "import MySQLdb\n",
    "db = MySQLdb.connect(host='103.22.220.149', #host\n",
    "                    user='hoheon', # user ID\n",
    "                    passwd='yonsei2018!', # password\n",
    "                    db='food', # database name\n",
    "                    use_unicode=True, # to define unicode\n",
    "                    charset='utf8') # to define unicode\n",
    "\n",
    "# make cursor object to excute all the SQL\n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_mysql.connection open to '103.22.220.149' at 0x2abe158>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_sql = \"SELECT * FROM EXAMPLE;\" \n",
    "cur.execute(select_sql)\n",
    "result = cur.fetchall()\n",
    "df = pd.DataFrame(np.array(result))\n",
    "df.to_csv('scraping.csv')\n",
    "cur.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file\n",
    "import pandas as pd\n",
    "df= pd.read_csv('scraping.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'흔히 위에 좋은 식품으로 잘 알려져 있는 양배추는 위를 보호하는 것 외에 비타민A와 칼슘, 철분, 칼륨이 풍부해 면역력을 강화하고, 항암효과도 있다.\\n바나나=바나나는 칼륨 함량이 높은 것으로 이미 잘 알려져 있다.\\n간편하게 먹을 수 있고, 쉽게 접할 수 있는 바나나는 열량이 낮고 1개당 422mg의 칼륨이 함유돼 있다.\\n우유가 고혈압 예방에 효과적인 것은 우유에 함유된 칼륨·칼슘·마그네슘이 혈압을 낮추기 때문이다.오쿠다 교수는 우유를 즐겨 마시면 소금을 적게 섭취하게 돼, 결과적으로 혈압이 낮아진다고 설명했다.고구마=겨울철 영양만점 간식으로 사랑받는 고구마도 칼륨이 풍부한 식품이다.\\n다이어트 식품으로도 사랑 받는 고구마는 작은 주먹 정도 크기에 약 540mg의 칼륨이 함유돼 있다.\\n또 시금치나 양배추 등 비타민K 함량이 높은 식품들은 비타민 K 길항제를 복용하는 환자들이 주의해야한다.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.summarization import summarize\n",
    "from newspaper import Article\n",
    "df.columns = ['INDEX','ID', 'KCD_CODE', 'TITLE', 'DATE', 'CONTENTS', 'PROVIDER', 'WRITER']\n",
    "sample='''[현대건강신문=채수정 기자] 날씨가 추운 겨울철은 고혈압 환자들의 건강관리에 더 주의를 기울여야 한다. 혈압은 상황에 따라 변하는데 특히 기온에 민감하게 반응한다. 몸이 찬 공기에 노출되면서 교감신경계 영향으로 혈관이 수축하기 때문이다. 연구에 따르면 기온이 1도 내려가면 수축기혈압은 1.3㎜Hg 상승한다고 알려져 있다.\n",
    "경희의료원 심장혈관센터 김원 교수는 “추위에 혈압 상승이 무서운 이유는 고혈압 자체보다 뇌출혈, 심근경색증, 뇌졸중 등의 합병증의 위험성이 크게 높아지기 때문”이라고 말한다.또한 겨울철은 실내 생활이 늘면서 운동량이 줄어들어 식생활 등 생활 관리에 더욱 신경써야 한다. 우리나라 고혈압 환자들에게 가장 어려운 것 중 하나가 바로 짜게 먹는 식습관이다. 소금을 줄여야 하지만, 오랫동안 익숙해진 식습관을 고치기는 쉽지 않다. \n",
    "고혈압, 당뇨 등 만성질환자들은 물론 적게 먹고 운동을 많이 하는 것이 가장 좋지만, 어떤 식품을 먹느냐 하는 것도 중요하다. 고혈압 환자들에게는 몸속의 나트륨을 배출하는 데 도움을 주는 칼륨이 풍부한 식품을 섭취하는 것이 도움이 된다.추운 겨울철 고혈압 환자들의 건강관리에 도움을 주는 칼륨이 풍부한 식품을 알아보았다.양배추=양배추는 우리 주위에서 가장 흔하고 저렴하게 먹을 수 있는 식재료라 무심코 지나치기 쉽지만, 최고의 슈퍼푸드다. \n",
    "흔히 위에 좋은 식품으로 잘 알려져 있는 양배추는 위를 보호하는 것 외에 비타민A와 칼슘, 철분, 칼륨이 풍부해 면역력을 강화하고, 항암효과도 있다. 또 저열량에 식이섬유도 풍부해 다이어트에도 효과만점이다.저장성이 좋아 겨울철에 구하기 쉬운 양배추는 저열량에 칼륨 함량도 높아 혈압 관리에도 도움이 된다. 바나나=바나나는 칼륨 함량이 높은 것으로 이미 잘 알려져 있다. 간편하게 먹을 수 있고, 쉽게 접할 수 있는 바나나는 열량이 낮고 1개당 422mg의 칼륨이 함유돼 있다. \n",
    "바나나의 칼륨은 나트륨 배출을 돕고 전해질 균형을 맞춰 혈압을 정상 수준으로 유지시킨다. 또 바나나의 펙틴 성분은 변비 개선에 도움을 주고, 마그네슘 성분도 풍부해 수분 균형을 유지시키고 눈 떨림 증상을 완화한다. \n",
    "시금치=겨울에 제철을 맞은 식재료 중 하나인 시금치도 칼륨이 풍부하다. 시금치 100g에는 약 800mg의 칼륨을 함유하고 있어 가격대비 최고다. 시금치에는 또  철, 인, 칼슘 등의 미네랄과 녹황색 채소 중에 비타민 A가 가장 많고 카로틴함량이 높아 이를 많이 섭취하면 암 발병을 억제하는 데 효과가 있다. 이 밖에 사포닌과 질 좋은 섬유질이 들어 있어 변비에 효과가 있고 철분과 엽산 덕분에 빈혈 예방에도 좋다.\n",
    "우유=완전 식품으로 불리는 우유도 칼륨 함량이 높다. 일본총합과학대학 건강영양과학과 나가코 오쿠다 교수에 따르면, 일본인을 대상으로 한 연구에서 우유를 많이 마시면 남녀 모두에서 수축기(최대) 혈압이 눈에 띄게 낮아지는 것으로 나타났다.\n",
    "우유가 고혈압 예방에 효과적인 것은 우유에 함유된 칼륨·칼슘·마그네슘이 혈압을 낮추기 때문이다.오쿠다 교수는 우유를 즐겨 마시면 소금을 적게 섭취하게 돼, 결과적으로 혈압이 낮아진다고 설명했다.고구마=겨울철 영양만점 간식으로 사랑받는 고구마도 칼륨이 풍부한 식품이다. 다이어트 식품으로도 사랑 받는 고구마는 작은 주먹 정도 크기에 약 540mg의 칼륨이 함유돼 있다. 특히 고구마는 식이섬유도 풍부해 장 활동을 개선하고 달콤한 맛에 칼로리도 낮다.\n",
    "칼륨 성분은 우리 몸속에 나트륨 배출을 도와 혈압을 안정시키는데 도움을 준다. 다만 고혈압이나 심장 질환에 흔히 쓰이는 약인 베타차단제를 먹고 있거나 신장 건강이 나쁘다면 칼륨의 지나친 섭취에 주의해야한다. \n",
    "체내 칼륨 수치가 지나치게 높을 경우 신장 기능이 약한 신장질환자들의 경우 혈액에 쌓여 심장 기능을 떨어뜨리기 때문이다. 또 시금치나 양배추 등 비타민K 함량이 높은 식품들은 비타민 K 길항제를 복용하는 환자들이 주의해야한다.'''\n",
    "summarize(sample, ratio=0.25, word_count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "sys.path.append('/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner')\n",
    "BASE_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/TF_IDF/'\n",
    "BASE_PATH2 = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner/'\n",
    "import pyTextMiner as ptm\n",
    "TOPIC_MODELING_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/Topic modeling/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = df.loc[:80000, 'CONTENTS']\n",
    "# 개행문자 제거\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'example.txt', mode='w')\n",
    "for doc in documents:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full txt file\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'example.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "# SAVE\n",
    "import json\n",
    "outputfile = open('preprocessing_result_11K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 80000 txt file\n",
    "documents = df.loc[:80000, 'CONTENTS']\n",
    "# 개행문자 제거\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'example.txt', mode='w')\n",
    "for doc in documents:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'example.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "# SAVE\n",
    "import json\n",
    "outputfile = open('preprocessing_result_8K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.base_any2vec:EPOCH - 1 : supplied example count (20001) did not equal expected count (304954)\n",
      "WARNING:gensim.models.base_any2vec:EPOCH - 2 : supplied example count (20001) did not equal expected count (304954)\n",
      "WARNING:gensim.models.base_any2vec:EPOCH - 3 : supplied example count (20001) did not equal expected count (304954)\n",
      "WARNING:gensim.models.base_any2vec:EPOCH - 4 : supplied example count (20001) did not equal expected count (304954)\n",
      "WARNING:gensim.models.base_any2vec:EPOCH - 5 : supplied example count (20001) did not equal expected count (304954)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-036aa19af25b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mmax_vocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    ) \n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Average sentence vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert to len 1 list\n",
    "word_list = []\n",
    "for sub in result:\n",
    "        word_list.extend(sub)\n",
    "\n",
    "# Word2Vec Model training\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(word_list, min_count=5, size = 100, window = 5, workers= 45, # ignore the voca that appear at least 5 times in the corpus\n",
    "                    sg=0,  # 0 : CBOW, 1: Skip-gram\n",
    "                    hs=0,  # 0: non-zero, neative sampling 1: hierachila softmax \n",
    "                    alpha= 0.01,\n",
    "                    min_alpha= 0.0001,\n",
    "                    max_vocab_size = 1000\n",
    "                   ) \n",
    "model.train(documents, total_examples=len(word_list), epochs=100, report_delay=0.1)\n",
    "\n",
    "# Average sentence vector\n",
    "index2word_set = set(model.wv.index2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
    "    words = sentence.split()\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['고혈압',\n",
       " '질환',\n",
       " '건강',\n",
       " '환자',\n",
       " '치료',\n",
       " '보험',\n",
       " '운동',\n",
       " '병원',\n",
       " '보장',\n",
       " '기자',\n",
       " '예방',\n",
       " '당뇨',\n",
       " '효과',\n",
       " '발생',\n",
       " '비만',\n",
       " '관리',\n",
       " '세',\n",
       " '증가',\n",
       " '혈압',\n",
       " '증상',\n",
       " '심장',\n",
       " '당뇨병',\n",
       " '만성',\n",
       " '보험료',\n",
       " '의료',\n",
       " '시',\n",
       " '결과',\n",
       " '원인',\n",
       " '혈관',\n",
       " '위험',\n",
       " '가입',\n",
       " '검사',\n",
       " '필요',\n",
       " '질병',\n",
       " '약',\n",
       " '지역',\n",
       " '몸',\n",
       " '암',\n",
       " '교수',\n",
       " '연구',\n",
       " '대상',\n",
       " '상태',\n",
       " '섭취',\n",
       " '진단',\n",
       " '생활',\n",
       " '.kr',\n",
       " '뇌졸중',\n",
       " '여성',\n",
       " '사망',\n",
       " '병',\n",
       " '사업',\n",
       " '치매',\n",
       " '음식',\n",
       " '노인',\n",
       " '.co',\n",
       " '검진',\n",
       " '조사',\n",
       " '개발',\n",
       " '유지',\n",
       " '국내',\n",
       " '사용',\n",
       " '식품',\n",
       " '상품',\n",
       " '진료',\n",
       " '진행',\n",
       " '보건',\n",
       " '지방',\n",
       " '치료제',\n",
       " '방법',\n",
       " '지급',\n",
       " '혈액',\n",
       " '부모',\n",
       " '시장',\n",
       " '이번',\n",
       " '보건소',\n",
       " '남성',\n",
       " '대표',\n",
       " '약물',\n",
       " '경제',\n",
       " '센터',\n",
       " '시작',\n",
       " '실시',\n",
       " '복용',\n",
       " '도움',\n",
       " '성분',\n",
       " '개선',\n",
       " '장애',\n",
       " '지속',\n",
       " '심근',\n",
       " '상담',\n",
       " '.com',\n",
       " '설명',\n",
       " '의약품',\n",
       " '의원',\n",
       " '제약',\n",
       " '감소',\n",
       " '교육',\n",
       " '김',\n",
       " '식',\n",
       " '신약',\n",
       " '스트레스',\n",
       " '이용',\n",
       " '있다',\n",
       " '제공',\n",
       " '체중',\n",
       " '조절',\n",
       " '기간',\n",
       " '고지혈증',\n",
       " '뇌',\n",
       " '의사',\n",
       " '주민',\n",
       " '운영',\n",
       " '지난해',\n",
       " '활동',\n",
       " '합병증',\n",
       " '전문',\n",
       " '계획',\n",
       " '습관',\n",
       " '영양',\n",
       " '하루',\n",
       " '신경',\n",
       " '임신',\n",
       " '기관',\n",
       " '신장',\n",
       " '부담',\n",
       " '정상',\n",
       " '정보',\n",
       " '세계',\n",
       " '뒤',\n",
       " '수면',\n",
       " '평가',\n",
       " '기준',\n",
       " '콜레스테롤',\n",
       " '처방',\n",
       " '혈당',\n",
       " '게',\n",
       " '주의',\n",
       " '뉴스',\n",
       " '국민',\n",
       " '동맥',\n",
       " '경색',\n",
       " '부족',\n",
       " '분석',\n",
       " '가족',\n",
       " '관심',\n",
       " '배포',\n",
       " '인기',\n",
       " '근육',\n",
       " '군',\n",
       " '요법',\n",
       " '변화',\n",
       " '심혈관',\n",
       " '한국',\n",
       " '유발',\n",
       " '발병',\n",
       " '박',\n",
       " '인해',\n",
       " '프로그램',\n",
       " '확인',\n",
       " '평소',\n",
       " '수준',\n",
       " '소금',\n",
       " '우리나라',\n",
       " '비용',\n",
       " '식사',\n",
       " '관계자',\n",
       " '정부',\n",
       " '뇌혈관',\n",
       " '작용',\n",
       " '전체',\n",
       " '신체',\n",
       " '영향',\n",
       " '팀',\n",
       " '발견',\n",
       " '평균',\n",
       " '행복',\n",
       " '이유',\n",
       " '가운데',\n",
       " '손상',\n",
       " '내과',\n",
       " '장기',\n",
       " '처음',\n",
       " '흡연',\n",
       " '임상',\n",
       " '서울',\n",
       " '성인',\n",
       " '납입',\n",
       " '발표',\n",
       " '나트륨',\n",
       " '나이',\n",
       " '조기',\n",
       " '심',\n",
       " '전국',\n",
       " '성인병',\n",
       " '예정',\n",
       " '경화',\n",
       " '교실',\n",
       " '연령',\n",
       " '요인',\n",
       " '상승',\n",
       " '악화',\n",
       " '수치',\n",
       " '음주',\n",
       " '전문가',\n",
       " '마련',\n",
       " '상황',\n",
       " '고령',\n",
       " '인구',\n",
       " '날',\n",
       " '부작용',\n",
       " '측정',\n",
       " '당',\n",
       " '복합',\n",
       " '차지',\n",
       " '금연',\n",
       " '학회',\n",
       " '적극',\n",
       " '아',\n",
       " '어린이',\n",
       " '오후',\n",
       " '정책',\n",
       " '노력',\n",
       " '동반',\n",
       " '관계',\n",
       " '자료',\n",
       " '협심증',\n",
       " '전문의',\n",
       " '쿠키',\n",
       " '마늘',\n",
       " '한식',\n",
       " '곶감',\n",
       " '경로당',\n",
       " '쌀',\n",
       " '뽕잎',\n",
       " '맛',\n",
       " '웃음',\n",
       " '황태',\n",
       " '밥',\n",
       " '어르신',\n",
       " '산모',\n",
       " '시범',\n",
       " '참여',\n",
       " '발효',\n",
       " '추진',\n",
       " '한과',\n",
       " '원주시',\n",
       " '장성',\n",
       " '시행',\n",
       " '보호',\n",
       " '채취',\n",
       " '태안',\n",
       " '쪽',\n",
       " '서부',\n",
       " '우울',\n",
       " '응답',\n",
       " '스낵',\n",
       " '과자',\n",
       " '잔',\n",
       " '판매',\n",
       " '일반',\n",
       " '남동구',\n",
       " '업소',\n",
       " '조리',\n",
       " '전통',\n",
       " '가격',\n",
       " '웰빙',\n",
       " '만족',\n",
       " '오리지널',\n",
       " '엿',\n",
       " '함량',\n",
       " '유림',\n",
       " '동의',\n",
       " '영국',\n",
       " '건창',\n",
       " '바이오',\n",
       " '음료',\n",
       " '잎']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7202667593955994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Caculate similariry\n",
    "from scipy import spatial\n",
    "# from spatial import distance\n",
    "s1_afv = avg_feature_vector(df.CONTENTS[1], model=model, num_features=100, index2word_set=index2word_set)\n",
    "s2_afv = avg_feature_vector(df.CONTENTS[2], model=model, num_features=100, index2word_set=index2word_set)\n",
    "sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Topic Modeling : Latent Dirichlet Allocation\n",
    "Topic modeling은 단어의 군집화. 기술적 분석임(Descriptive analaytics)\n",
    "전체의 문헌의 N개의 토픽을 구성하는 단어를 추출하는 방법. 사후 추론방법. \n",
    "* D: 문서 전체의 개수\n",
    "* K: 전체 토픽 수 (하이퍼 파라미터)\n",
    "* N: d번째 문서의 단어수 (W)\n",
    "* Z(d,n) : \n",
    "* Phi: Term Topic matrix (하이퍼파라미터 beta에 영향을 받는다. phi는 디리클레분포를 따른다는 가정)\n",
    "* Ceta(d,K): d번째 문헌에서 k번째 토픽의 확률들.\n",
    "* W(d,n): d번째 문헌에 있는, n번째의 단어\n",
    "* beta: 배타. \n",
    "\n",
    "\n",
    "문헌은 여러개의 topic으로 이루어져있다는게 가정임\n",
    "최적화하는 K값은 토픽의의 similarity는 낮아야한다.\n",
    "각각의 토픽끼리는 cohesive해야한다.\n",
    "Variannal inference: KL divergence을 계산해서 (dissimilarity을 찾는다)\n",
    "\n",
    "* perplexity가 낮을 수록 좋음 (순도): 늘 좋진 않다. 주관적으로 할 필요가 있음.\n",
    "* 잘 나온 토픽들로만 생성한다는게 이해가 안안되는데\n",
    "\n",
    "*LDA inference: 새로운 문헌이 들어올 때, topic을 추론하는 방법\n",
    "*LDA: word x topic * topic x document\n",
    "\n",
    "토픽을 추정하는 방법\n",
    "* Variational inference: 시간이 오래걸려서 잘 안씀.\n",
    "* Gibbs sampling: 토픽을 추론하기위해서, 깁스 방법으로 샘플링해서, 샘플링 한 단어를가지고 추론을 함. 확률적 샘플링이라 조금씩 달라질 수 있다.\n",
    "\n",
    "1. 포아송 분포로부터 문헌의 길이를 N을 선택\n",
    "2. 알파를 매개변수로 하는 주제 분포를 ceta을 선택한다 (alpha: 알아서 학습을 통해 추정이 된다)\n",
    "3. 임의로 주제를 정해서 \n",
    "\n",
    "다항 토픽모델링:  T*F: feature space을 변수로 넣고, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "import json\n",
    "with open('preprocessing_result_80K.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "        \n",
    "# Convert input data format\n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e6ee9a8816428e94d4699334ea06d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "K=20\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "from tqdm import tnrange\n",
    "for d in tnrange(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def document_count(d, documents, document_topics):\n",
    "    word_list_in_documents = documents[d]\n",
    "    topic = document_topics[d]\n",
    "    global document_topic_counts\n",
    "    global topic_word_counts\n",
    "    global topic_counts\n",
    "    for word in word_list_in_documents:\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "    print(d, word, topic)\n",
    "#     return (document_topic_counts, topic_word_counts, topic_counts)\n",
    "    \n",
    "    # topic_counts: 토픽별로 있는 단어의 빈도\n",
    "    # document_topic_counts: 각 토픽별로 있는 단어의 개수. 위의 토픽이 될만한 단어들을 카운트\n",
    "    # topic_counts: topic 에따른 총 단어수 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2972831"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5f3b79c827eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_cores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use 53 CPUs / total 56 CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m                                      \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_topics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count() - 3 ; print(num_cores) # use 53 CPUs / total 56 CPU \n",
    "\n",
    "results = Parallel(n_jobs=num_cores)(delayed(document_count)(d, word, topic) \\\n",
    "                                     for d in range(1) \\\n",
    "                                     for word, topic in zip(documents[d], document_topics[d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "num_iteration = 100\n",
    "\n",
    "def processInput(i, *my_tuple):\n",
    "    global document_topic_counts\n",
    "    global topic_word_counts\n",
    "    global topic_counts\n",
    "    global document_lengths\n",
    "    global new_topic\n",
    "    document_topic_counts[d][topic] -= 1\n",
    "    topic_word_counts[topic][word] -= 1\n",
    "    topic_counts[topic] -= 1\n",
    "    document_lengths[d] -= 1\n",
    "    new_topic = choose_new_topic(d, word)\n",
    "    document_topics[d][i] = new_topic\n",
    "    document_topic_counts[d][new_topic] += 1\n",
    "    topic_word_counts[new_topic][word] += 1\n",
    "    topic_counts[new_topic] += 1\n",
    "    document_lengths[d] += 1\n",
    " \n",
    "num_cores = multiprocessing.cpu_count() - 3 # total 56 CPU\n",
    "print(num_cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processInput(1, (wo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD\n",
    "import json\n",
    "with open('preprocessing_result_80K.json', 'r') as f:\n",
    "    result = json.load(f)\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "        \n",
    "# Convert input data format\n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list \n",
    "\n",
    "random.seed(0)\n",
    "K=20\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "\n",
    "from tqdm import tnrange\n",
    "for iter in tnrange(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "            \n",
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})\n",
    "import json\n",
    "outputfile = open('TopicModeling_80K.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(i, d, *my_tuple):\n",
    "    '''\n",
    "    input: i: Sequence\n",
    "    my_tuple: enumerate(zip(documents[d], document_topics[d]))\n",
    "    '''\n",
    "    document_topic_counts[d][topic] -= 1\n",
    "    topic_word_counts[topic][word] -= 1\n",
    "    topic_counts[topic] -= 1\n",
    "    document_lengths[d] -= 1\n",
    "    new_topic = choose_new_topic(d, word)\n",
    "    document_topics[d][i] = new_topic\n",
    "    document_topic_counts[d][new_topic] += 1\n",
    "    topic_word_counts[new_topic][word] += 1\n",
    "    topic_counts[new_topic] += 1\n",
    "    document_lengths[d] += 1\n",
    "    print(document_lengths)\n",
    "#     print(dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1928), ('질환', 1857), ('환자', 1585), ('건강', 1532), ('치료', 1199), ('운동', 886), ('보험', 865), ('병원', 766), ('보장', 747), ('당뇨', 700)]\n",
      "1th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1804), ('질환', 1797), ('환자', 1580), ('건강', 1455), ('치료', 1192), ('보험', 860), ('운동', 845), ('병원', 753), ('보장', 747), ('예방', 720)]\n",
      "2th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1875), ('질환', 1803), ('건강', 1530), ('환자', 1455), ('치료', 1247), ('보험', 885), ('운동', 881), ('병원', 777), ('보장', 750), ('예방', 734)]\n",
      "3th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 2003), ('질환', 1807), ('건강', 1570), ('환자', 1484), ('치료', 1269), ('보험', 876), ('운동', 797), ('병원', 755), ('당뇨', 718), ('기자', 690)]\n",
      "4th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1961), ('질환', 1770), ('건강', 1543), ('환자', 1451), ('치료', 1283), ('보험', 895), ('운동', 853), ('병원', 781), ('보장', 755), ('당뇨', 705)]\n",
      "5th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1984), ('질환', 1834), ('건강', 1582), ('환자', 1461), ('치료', 1238), ('보험', 938), ('운동', 852), ('보장', 759), ('병원', 712), ('예방', 710)]\n",
      "6th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1954), ('질환', 1708), ('건강', 1563), ('환자', 1504), ('치료', 1194), ('보험', 928), ('운동', 878), ('병원', 784), ('보장', 737), ('기자', 692)]\n",
      "7th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1896), ('질환', 1756), ('건강', 1498), ('환자', 1467), ('치료', 1255), ('보험', 917), ('운동', 839), ('병원', 732), ('보장', 716), ('발생', 707)]\n",
      "8th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1995), ('질환', 1743), ('건강', 1528), ('환자', 1475), ('치료', 1295), ('보험', 889), ('운동', 872), ('보장', 734), ('병원', 721), ('당뇨', 691)]\n",
      "9th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1945), ('질환', 1863), ('건강', 1455), ('환자', 1413), ('치료', 1227), ('보험', 916), ('운동', 831), ('병원', 779), ('당뇨', 728), ('보장', 727)]\n",
      "10th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 2054), ('질환', 1787), ('환자', 1526), ('건강', 1500), ('치료', 1303), ('보험', 894), ('운동', 872), ('병원', 753), ('보장', 705), ('예방', 698)]\n",
      "11th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1883), ('질환', 1804), ('건강', 1608), ('환자', 1470), ('치료', 1163), ('보험', 934), ('운동', 888), ('병원', 777), ('보장', 760), ('기자', 704)]\n",
      "12th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1920), ('질환', 1779), ('건강', 1472), ('환자', 1459), ('치료', 1315), ('보험', 902), ('운동', 817), ('보장', 796), ('병원', 762), ('발생', 687)]\n",
      "13th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1905), ('질환', 1779), ('환자', 1566), ('건강', 1529), ('치료', 1237), ('보험', 928), ('운동', 870), ('병원', 734), ('보장', 734), ('기자', 707)]\n",
      "14th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1979), ('질환', 1755), ('환자', 1471), ('건강', 1467), ('치료', 1204), ('운동', 889), ('보험', 883), ('보장', 804), ('병원', 723), ('당뇨', 711)]\n",
      "15th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1952), ('질환', 1763), ('건강', 1547), ('환자', 1407), ('치료', 1176), ('보험', 927), ('운동', 832), ('병원', 768), ('보장', 740), ('예방', 680)]\n",
      "16th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1913), ('질환', 1832), ('건강', 1504), ('환자', 1453), ('치료', 1154), ('보험', 870), ('운동', 861), ('기자', 739), ('보장', 733), ('병원', 719)]\n",
      "17th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1947), ('질환', 1709), ('건강', 1559), ('환자', 1437), ('치료', 1213), ('보험', 872), ('운동', 861), ('보장', 768), ('병원', 724), ('예방', 689)]\n",
      "18th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1933), ('질환', 1809), ('건강', 1539), ('환자', 1524), ('치료', 1218), ('보험', 905), ('운동', 878), ('병원', 787), ('보장', 776), ('예방', 688)]\n",
      "19th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('고혈압', 1886), ('질환', 1786), ('건강', 1544), ('환자', 1441), ('치료', 1233), ('보험', 910), ('운동', 868), ('병원', 740), ('보장', 707), ('예방', 698)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topic_word_counts)):\n",
    "    print('{}th topic'.format(i), '-'*150)\n",
    "    print(sorted(dict(topic_word_counts[i]).items(), key=lambda x:x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 14), (3, 13), (19, 13), (17, 12), (1, 10)]\n"
     ]
    }
   ],
   "source": [
    "# 0 번문서의 topic의 빈도\n",
    "print(sorted(document_topic_counts[0].items(), key= lambda x:x[1], reverse=True)[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c168cb2b1874988b511cbd77ebe6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=502772), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 각 번문서의 top one pick 주제\n",
    "import numpy as np\n",
    "empty_topic_list = []\n",
    "for i in tnrange(len(documents)):\n",
    "    try:\n",
    "        empty_topic_list.append(sorted(document_topic_counts[i].items(), key= lambda x:x[1], reverse=True)[0:1][0][0])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "outputfile = open('topic', 'w+')\n",
    "json.dump(topic_word_counts, outputfile)\n",
    "output_file = open('doc_topic_list', 'w+')\n",
    "json.dump(empty_topic_list, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('doc_topic_list', 'r') as f:\n",
    "    my_dic = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['유전',\n",
       " '식습관',\n",
       " '발생',\n",
       " '요인',\n",
       " '조기',\n",
       " '사망',\n",
       " '확률',\n",
       " '방치',\n",
       " '질문',\n",
       " '아버지',\n",
       " '혈압',\n",
       " '161',\n",
       " '고혈압',\n",
       " '약',\n",
       " '이유',\n",
       " '고혈압',\n",
       " '약',\n",
       " '시작',\n",
       " '머리',\n",
       " '약인',\n",
       " '고혈압',\n",
       " '관리',\n",
       " '관리',\n",
       " '잘못',\n",
       " '부작용',\n",
       " '답',\n",
       " '고혈압',\n",
       " '만성',\n",
       " '동맥',\n",
       " '혈압',\n",
       " '상태',\n",
       " '운동',\n",
       " '직후',\n",
       " '일시',\n",
       " '혈압',\n",
       " '고혈압',\n",
       " '마음',\n",
       " '측정',\n",
       " '연속',\n",
       " '혈압',\n",
       " '그때',\n",
       " '고혈압',\n",
       " '진단',\n",
       " '혈압',\n",
       " '최',\n",
       " '고혈압',\n",
       " '최',\n",
       " '저혈압',\n",
       " '고혈압',\n",
       " '미국',\n",
       " '국립보건원',\n",
       " '세계보건기구',\n",
       " '최',\n",
       " '고혈압',\n",
       " '최',\n",
       " '저혈압',\n",
       " '고혈압',\n",
       " '정의',\n",
       " '최',\n",
       " '고혈압',\n",
       " '최',\n",
       " '저혈압',\n",
       " '고혈압',\n",
       " '단계',\n",
       " '관리',\n",
       " '정상',\n",
       " '이란 판',\n",
       " '게',\n",
       " '고혈압',\n",
       " '원인',\n",
       " '갑상선',\n",
       " '기능',\n",
       " '항진증',\n",
       " '신장',\n",
       " '질환',\n",
       " '경구',\n",
       " '임제',\n",
       " '복용',\n",
       " '레닌',\n",
       " '분비',\n",
       " '종양',\n",
       " '등등',\n",
       " '이유',\n",
       " '치료',\n",
       " '원인',\n",
       " '제거',\n",
       " '하지',\n",
       " '원인',\n",
       " '고혈압',\n",
       " '전체',\n",
       " '환자',\n",
       " '치료법',\n",
       " '비교',\n",
       " '이유',\n",
       " '고혈압',\n",
       " '발생',\n",
       " '요인',\n",
       " '유전',\n",
       " '식습관',\n",
       " '이야기',\n",
       " '고혈압',\n",
       " '비만',\n",
       " '술',\n",
       " '가족',\n",
       " '고혈압',\n",
       " '연관',\n",
       " '고혈압',\n",
       " '증상',\n",
       " '치료',\n",
       " '이유',\n",
       " '당장',\n",
       " '불편',\n",
       " '게',\n",
       " '고혈압',\n",
       " '조기',\n",
       " '사망',\n",
       " '확률',\n",
       " '질환',\n",
       " '사인',\n",
       " '심장',\n",
       " '질환',\n",
       " '뇌출혈',\n",
       " '사망',\n",
       " '원인',\n",
       " '신장',\n",
       " '손상',\n",
       " '단백질',\n",
       " '혈액',\n",
       " '소변',\n",
       " '신부전',\n",
       " '이행',\n",
       " '세계보건기구',\n",
       " '고혈압',\n",
       " '기준',\n",
       " '고혈압',\n",
       " '병',\n",
       " '할',\n",
       " '고혈압',\n",
       " '치료',\n",
       " '방침',\n",
       " '저염식',\n",
       " '약물',\n",
       " '이뇨제',\n",
       " '아드레날린',\n",
       " '차단',\n",
       " '혈관',\n",
       " '확장',\n",
       " '칼슘',\n",
       " '차단',\n",
       " '지오',\n",
       " '텐',\n",
       " '신',\n",
       " 'II',\n",
       " '효소',\n",
       " '억제',\n",
       " '약',\n",
       " '요즘',\n",
       " '각광',\n",
       " '고혈압',\n",
       " '약',\n",
       " '시작',\n",
       " '평생',\n",
       " '약',\n",
       " '사이',\n",
       " '심부전',\n",
       " '뇌출혈',\n",
       " '신부전',\n",
       " '사망',\n",
       " '감수',\n",
       " '쪽',\n",
       " '필요',\n",
       " '요즘',\n",
       " '고혈압',\n",
       " '약',\n",
       " '게',\n",
       " '아침',\n",
       " '아침밥',\n",
       " '고혈압',\n",
       " '병',\n",
       " '이동수',\n",
       " '전주',\n",
       " '은',\n",
       " '병원',\n",
       " '원장']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k \n",
    "[j for sub in result[2:3] for term in sub for j in term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Filtered topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword 수정\n",
    "f = open(BASE_PATH2+'stopwordsKor.txt','a')\n",
    "for stopword_add in sorted_x[:30]:\n",
    "    f.write(stopword_add[0]+'\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic 별 단어 리스트\n",
    "# 전처리 리스트 불러오기\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from collections import Counter\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "sys.path.append('/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner')\n",
    "BASE_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/TF_IDF/'\n",
    "BASE_PATH2 = '/home/user/jupyter/Hoheon/2019_01/Text mining/data/pyTextMiner/'\n",
    "import pyTextMiner as ptm\n",
    "TOPIC_MODELING_PATH = '/home/user/jupyter/Hoheon/2019_01/Text mining/Topic modeling/'\n",
    "os.chdir(TOPIC_MODELING_PATH)\n",
    "\n",
    "\n",
    "with open('topic', 'r') as f:\n",
    "    topic = json.load(f)\n",
    "with open('doc_topic_list', 'r') as f:\n",
    "    topic_label = json.load(f)\n",
    "\n",
    "df= pd.read_csv('scraping.csv')\n",
    "# topic\n",
    "df['topic'] = np.array(topic_label)[:len(df)]\n",
    "reduced_df = df[:80000]\n",
    "reduced_df.columns = ['INDEX','ID', 'KCD_CODE', 'TITLE', 'DATE', 'CONTENTS', 'PROVIDER', 'WRITER','TOPIC']\n",
    "# 기사 필터링----------\n",
    "# 9번: 지역사회관리\n",
    "# 10번: 식이요법\n",
    "# 19번: 일반 고혈압정보\n",
    "reduced_df = reduced_df.loc[(reduced_df.TOPIC == 9) | (reduced_df.TOPIC == 10) |(reduced_df.TOPIC == 19)]\n",
    "reduced_df =  reduced_df.loc[:,'CONTENTS']\n",
    "reduced_df = reduced_df.values\n",
    "documents_txt = open(file= TOPIC_MODELING_PATH+'reduced_news_result.txt', mode='w')\n",
    "for doc in reduced_df:\n",
    "    doc = doc.replace('\\n','')\n",
    "    documents_txt.write(doc+'\\n')\n",
    "documents_txt.close()\n",
    "# 전처리\n",
    "corpus = ptm.CorpusFromFile(TOPIC_MODELING_PATH + 'reduced_news_result.txt') # load csv file\n",
    "pipeline = ptm.Pipeline(ptm.splitter.NLTK(), \n",
    "                        ptm.tokenizer.Komoran(),\n",
    "                        ptm.helper.POSFilter('NN*'),\n",
    "                        ptm.helper.SelectWordOnly(),\n",
    "                        ptm.helper.StopwordFilter(file=BASE_PATH2+'stopwordsKor.txt')\n",
    "                       )\n",
    "result = pipeline.processCorpus(corpus)\n",
    "outputfile = open('filter_news_preprocessed.json', 'w+')\n",
    "json.dump(result, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c310add570cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m document_topics = [[random.randrange(K) for word in document]\n\u001b[0;32m---> 39\u001b[0;31m                     for document in documents]\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mdocument_topic_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtopic_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-c310add570cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m document_topics = [[random.randrange(K) for word in document]\n\u001b[0;32m---> 39\u001b[0;31m                     for document in documents]\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mdocument_topic_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtopic_word_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-c310add570cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m document_topics = [[random.randrange(K) for word in document]\n\u001b[0m\u001b[1;32m     39\u001b[0m                     for document in documents]\n\u001b[1;32m     40\u001b[0m \u001b[0mdocument_topic_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/random.py\u001b[0m in \u001b[0;36m_randbelow\u001b[0;34m(self, n, int, maxsize, type, Method, BuiltinMethod)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;34m\"Return a random int in the range [0,n).  Raises ValueError if n==0.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mrandom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Only call self.getrandbits if the original random() builtin method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 서프토픽모델링 \n",
    "flatten_list= [] \n",
    "for sub in result:\n",
    "    document = []\n",
    "    for sub_sub in sub:\n",
    "        document.extend(sub_sub)\n",
    "    flatten_list.append(document)\n",
    "documents = flatten_list \n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + V * beta))\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])\n",
    "\n",
    "def sample_from(weights):\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random()\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w\n",
    "        if rnd <= 0:\n",
    "            return i\n",
    "\n",
    "random.seed(0)\n",
    "K=10\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                    for document in documents]\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "document_lengths = [len(document) for document in documents]\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "V = len(distinct_words)\n",
    "D = len(documents)\n",
    "\n",
    "from tqdm import tnrange\n",
    "for d in tnrange(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1\n",
    "        \n",
    "        \n",
    "from tqdm import tnrange\n",
    "for iter in tnrange(1500):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1\n",
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_11K =dict({\"document_length:\": document_lengths, \"topic_word_counts\":topic_word_counts, \"topic_counts\":topic_counts, \"document_topic_counts\":document_topic_counts, \"topic_counts\":topic_counts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'TopicModeling_80K.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-196199faa321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutputfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TopicModeling_80K.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_11K\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'TopicModeling_80K.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "outputfile = open('TopicModeling_80K.json', 'w+')\n",
    "json.dump(obj_11K, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('음식', 11448), ('효과', 8707), ('비타민', 8147), ('성분', 7566), ('섭취', 7481), ('건강', 7042), ('식품', 6662), ('예방', 6432), ('몸', 5840), ('맛', 5198)]\n",
      "1th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('환자', 12064), ('검사', 11582), ('질환', 11188), ('치매', 9945), ('검진', 8408), ('병', 6731), ('암', 6436), ('신장', 5355), ('기능', 5314), ('진단', 5116)]\n",
      "2th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('의료', 18433), ('병원', 10476), ('환자', 10011), ('진료', 9993), ('사업', 5013), ('원격', 4853), ('서비스', 4833), ('센터', 4535), ('기관', 4361), ('의원', 4261)]\n",
      "3th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('나트륨', 9538), ('건강', 8404), ('조사', 8399), ('섭취', 6742), ('소금', 5480), ('식품', 4921), ('고혈압', 4867), ('결과', 3932), ('질환', 3552), ('지역', 3225)]\n",
      "4th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('건강', 30400), ('관리', 15100), ('보건소', 14353), ('고혈압', 13150), ('질환', 12487), ('예방', 11327), ('교육', 10132), ('사업', 9313), ('주민', 9136), ('운영', 9000)]\n",
      "5th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('수술', 16026), ('운동', 10545), ('통증', 9092), ('환자', 8314), ('관절', 7482), ('척추', 6449), ('질환', 5649), ('시술', 5523), ('허리', 4710), ('신경', 4419)]\n",
      "6th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('비만', 21536), ('연구', 10989), ('지방', 10244), ('여성', 10048), ('결과', 9264), ('건강', 9235), ('운동', 8925), ('체중', 8704), ('임신', 7615), ('위험', 7506)]\n",
      "7th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('생산', 3513), ('재배', 3488), ('효과', 3405), ('개발', 3400), ('맛', 3362), ('지역', 3289), ('제품', 3059), ('대표', 2878), ('판매', 2512), ('고혈압', 2497)]\n",
      "8th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('혈관', 20341), ('질환', 19428), ('고혈압', 17745), ('심장', 17369), ('환자', 15562), ('뇌졸중', 15246), ('혈압', 14670), ('발생', 10152), ('위험', 8338), ('심근', 8316)]\n",
      "9th topic ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[('수면', 8345), ('질환', 8147), ('몸', 6015), ('환자', 5694), ('발생', 5217), ('원인', 4213), ('주의', 4065), ('스트레스', 3945), ('피부', 3850), ('복용', 3793)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(topic_word_counts)):\n",
    "    print('{}th topic'.format(i), '-'*150)\n",
    "    print(sorted(dict(topic_word_counts[i]).items(), key=lambda x:x[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
